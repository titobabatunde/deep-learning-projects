# -*- coding: utf-8 -*-
"""HW2P2_Starter_F23_edited.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q2ydFjVmNE0fIhuWnZ-dVYYR4dGehidO

# HW2P2: Face Classification and Verification

Congrats on coming to the second homework in 11785: Introduction to Deep Learning. This homework significantly longer and tougher than the previous homework. You have 2 sub-parts as outlined below. Please start early!


*   Face Recognition: You will be writing your own CNN model to tackle the problem of classification, consisting of 7001 identities
*   Face Verification: You use the model trained for classification to evaluate the quality of its feature embeddings, by comparing the similarity of known and unknown identities

Common errors which you may face in this homeworks (because of the size of the model)


*   CUDA Out of Memory (OOM): You can tackle this problem by (1) Reducing the batch size (2) Calling `torch.cuda.empty_cache()` and `gc.collect()` (3) Finally restarting the runtime

# Preliminaries
"""

import torch
from torchsummary import summary
import torchvision #This library is used for image-based operations (Augmentations)
import os
import gc
from tqdm import tqdm
from PIL import Image
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
import glob
import wandb
import matplotlib.pyplot as plt
DEVICE = 'cuda:1' if torch.cuda.is_available() else 'cpu'
print("Device: ", DEVICE)
import subprocess
import argparse
import torch.nn.functional as F


"""# TODOs
As you go, please read the code and keep an eye out for TODOs!

# Download Data from Kaggle
"""

# TODO: Use the same Kaggle code from HW1P2
# install Kaggle's API

# !kaggle competitions download -c 11-785-f23-hw2p2-classification
# !unzip -qo '11-785-f23-hw2p2-classification.zip' -d '/mnt/nvme/home/bbabatun/IDL/HW2P2/data'

# !kaggle competitions download -c 11-785-f23-hw2p2-verification
# !unzip -qo '11-785-f23-hw2p2-verification.zip' -d '/mnt/nvme/home/bbabatun/IDL/HW2P2/data'

"""# Configs

# Notes
Learn to build a CNN-based architecture for face classification and face verification
(1) Face Classification
    (I) Closed set multiclass classification problem where the subjects in the
        test set have also been seen in the training set
    (II) The embeddings for the subject be linearly separable from each other
    (III) A face classifier that can extract feature vectors from face images:
          (i) Feature extractor: Train model to learn facial features (skin tone
              , hair color, nose size, etc) from an image of a face and represent
              them as fixed length features vectors (face embeddings).
          (ii) Classification layer: The feature vector obtained at the end will
               be passed through an MLP to classify it among N categories, and
               use cross-entropy loss for optimization. The feature vectors
               obtained after training can then be used for the verification.
    (IV) For multi-class classifiction: the input to your system is a face's image
         and your model needs to predic the ID of the face.
         (*) If the dataset contains M total images that belong to N different people
             (where M>N), your model needs to produce "good" face embeddings
(2) Face Verification
    (I) Determining whether two face images are of the same person, without
        knowing how the person is
    (II) You are given an examplar of a category of data (face) and an unknown
         instance, and you must determine if the two are from the same class
    (III) Train a model to extract discriminative feature vectors from images,
          which have the property that feature vectors from any two images that
          belong to the same person are close than feature vectors derived from
          images of two different persons
    (IV) Given a pair of facial images, extract feature vectors from both and
         compute their similarity. If similarity exceeds a threshold, we declare
         a match.
    (V) A verification system that computes the similarity between feature
        vectors of two images. Here is a simple verification system:
        (i) Extracting feature vectors from image 1 -> fvector1
        (ii) Extracting feature vectors from image 1 -> fvector2
        (iii) fvector1, fvector2 -> similarity metric -> similarity score
        (iv) In actuality, we compare each unknown identity to all the known
             identities and then decide whether this unknown identity matches any
             known identity using a threshold method, and predic the known
             identity with the highest similarity score
(3) Data description
    (I) Dataset used is a subset of the VGGFace2 dataset. The images have been
        resized to 224 x 224 pixels.
    (II) The classification dataset consists of 7,001 identities that are
         balanced so that each class has the equal number of training images.
    (III) The verification dataset consists of 1080 identities that are split
          into 360 for the Dev-set and 720 for the Test-set. Each of these
          unknown identities will need to be mapped to either one of the 960 known
          identities, or to n000000, the "no correspondence" label for the
          remaining 120 identities.

(4) Dataset Class - ImageFolder
    (I) We will be using the ImageFolder class from the torchvision library
        and passing it the path to the training and validation dataset
    (II) Images in subfolder classification_data are arranged ina way that is
         compatible with this dataset class.
    (III) The ImageFolder class will automatically infer the labels and make a
          dataset object, which we can then pass on to the dataloader
    (IV) Remember to also pass the image tranforms to the dataset class for doing
         data augmentation
         (*) torchvision.transforms is designed for image transformations such as:
             resizing, cropping, flipping, rotating, adjusting brightness/constrast,
             normalizing pixel values, and converting images to tensors.
         (*) image transformations provide key advantages (keep in minda that
             these should reflect what you expect in real world (ie. no vertical
             flips)):
             (^) More data
             (^) Preventing overfitting (reduce memorization of specific images)
             (^) Invariance (irrespective of orientation or position)
             (^) Better Generalization (enhance performace on unseen data)

(5) File Structure
    (*) Each sub-folder in train, dev, and test contains images of one person,
        and the name of that sub-folder represents their ID
        (i) train: use the train set to train your model both for the
            classification and verification task.
        (ii) dev: use dev set to validate the classification accuracy.
        (iii) test: assign IDs for images in the range of [0,7000] in test and
              submit result. ImageFolder dataset by default maps each class to
              such an ID.
    (*) classification_sample_submission.csv: this is a samle submission file for
        classification where the first column is the image file names and the
        second column is the image label.
    (*) For the verification dataset folder:
        (i) known: the directory of all 960 known identities.
        (ii) unknown_test: the directory containing images for Verification Test.
             720 images of unknown identites.
        (iii) unknown_dev: the directory containing 360 images of uknown
              identities, which you are given the ground truth mapping for.
        (iv) verification_dev.csv: list of ground truth identity labels
             (mapped to a known identity in folder or "no correspondence" label).
        (v) verification_sample_submission.csv: this is a sample submission file
            for face verification. The first column is the index of the image
            files and the second column is the label of each image.
    (*) classification acc = # correctly classified images/total images
    (*) verification acc =  # correctly match unknown identities/ total known identities

(6) Create deeper layers with residual networks (resnets)
    (*) Skip connections allow us to take the activations of one layer and suddenly
        feed it to another layer, even much deeper in the network.
    (*) Resnets are made of residual blocks, which are a set of layers that are
        connected to each other, and the input of the first layer is added to the
        output of the last layer in the block (residual connection).
        There are several other blocks that make use of residual blocks and
        connections:
        MobilNet, ResNet, and ConvNet, ConvNeXt, etc (you can combine different blocks)

(7) Similarity metric
    (*) For each unknown identity, you will have to predics the known identity that
        is corresponds with (i.e. with highest similarity value) or, if the
        similarity score is below the threshold, predic that it is not represented
        in the known set.
(8) Try other loss functions: center-loss, triplet-loss, pair-wise loss, LM, L_GM,...
    (*) pair-wise loss -> look into triplet loss
"""



# You can do this with ImageFolder as well, but it requires some tweaking
class ClassificationTestDataset(torch.utils.data.Dataset):

    def __init__(self, data_dir, transforms):
        self.data_dir   = data_dir
        self.transforms = transforms

        # This one-liner basically generates a sorted list of full paths to each image in the test directory
        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        return self.transforms(Image.open(self.img_paths[idx]))
# end def

"""## Data visualization"""

# Visualize a few images in the dataset
# You can write your own code, and you don't need to understand the code
# It is highly recommended that you visualize your data augmentation as sanity check
def DataVisualize(TRAIN_DIR, train_transforms, config):
    r, c    = [5, 5]
    fig, ax = plt.subplots(r, c, figsize= (15, 15))

    k       = 0
    dtl     = torch.utils.data.DataLoader(
        dataset     = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms), # dont wanna see the images with transforms
        batch_size  = config['batch_size'],
        shuffle     = True,
    )

    for data in dtl:
        x, y = data

        for i in range(r):
            for j in range(c):
                img = x[k].numpy().transpose(1, 2, 0)
                ax[i, j].imshow(img)
                ax[i, j].axis('off')
                k+=1
        # end for loops
        plt.savefig('all_images.png', bbox_inches='tight', pad_inches=0.1)
        break

    del dtl
# end def
"""
ResNet: Residual Blocks
7x7 (filter/kernels), convolution, 64 channels, stride 2
3x3 max pooling with strid of 2
for 34 layer:
    (3) 3x3 conv 64 residual blocks
    (4) 3x3 conv 128 residual blocks
    (6) 3x3 conv 256 residual blocks
    (3) 3x3 conv 512 residual blocks
    (*) 1X1 average pooling
    (*) softmax
"""
class ConvABlock(torch.nn.Sequential):
    """
    Simple convolution block with choice of activation:
    (1) ReLU
    (2) GeLU
    (3) PReLU
    Stride 1 is often used when you want to maintain the
    spatial dimensions (height, width) of the feature maps
    """
    def __init__(self, inchannels, outchannels, kernel=3, stride=1, groups=1, dropout=0.0, activation="relu"):
        padding = (kernel - 1) // 2
        activations = {
            "relu" : torch.nn.ReLU(inplace=True),
            "prelu": torch.nn.PReLU(num_parameters=outchannels),
            "relu6" : torch.nn.ReLU6(inplace=True),
            "gelu": torch.nn.GELU(),
        }
        super(ConvABlock, self).__init__(
            # bias=False : assuming bias is not learned and reduces learnable parameters
            torch.nn.Conv2d(in_channels=inchannels, 
                            out_channels=outchannels, 
                            kernel_size=kernel, 
                            stride=stride, 
                            padding=padding, 
                            groups=groups, 
                            bias=False),
            torch.nn.BatchNorm2d(outchannels),
            torch.nn.Dropout2d(p=dropout),
            activations.get(activation, torch.nn.ReLU(inplace=True)) # True to save memory
        )
    # end def
# end class

class ResidualBlock(torch.nn.Module):
    def __init__(self, inchannels, interchannels, kernel=3, stride=1, activation="relu"):
        # input channels
        # internal channels
        super(ResidualBlock, self).__init__()
        self.convblock1 = ConvABlock(inchannels=inchannels,
                                     outchannels=interchannels,
                                     kernel=kernel,
                                     stride=stride,
                                     activation=activation)
        self.convblock2 = ConvABlock(inchannels=interchannels,
                                     outchannels=interchannels,
                                     kernel=kernel,
                                     stride=1,
                                     activation=activation)  
    # end def

    def forward(self, x):
        out = self.convblock1(x)
        out = self.convblock2(out)
        out += x
        return out
    # end def
# end class

class BottleNeckBlock(torch.nn.Module):
    def __init__(self, inchannels, interchannels, dropout, kernel=3, stride=1, activation="relu"):
        # input channels
        # internal channels
        super(BottleNeckBlock, self).__init__()
        self.convblock1 = ConvABlock(inchannels=inchannels,
                                     outchannels=interchannels,
                                     kernel=1,
                                     stride=stride,
                                     dropout=dropout,
                                     activation=activation)
        self.convblock2 = ConvABlock(inchannels=interchannels,
                                     outchannels=interchannels,
                                     kernel=kernel,
                                     stride=stride,
                                     dropout=dropout,                                
                                     activation=activation)  
        self.convblock3 = ConvABlock(inchannels=interchannels,
                                     outchannels=interchannels*4,
                                     kernel=1,
                                     stride=stride,
                                     dropout=dropout,                                     
                                     activation=activation)                                  
    # end def

    def forward(self, x):
        out = self.convblock1(x)
        out = self.convblock2(out)
        out = self.convblock3(out)
        out += x
        return out
    # end def
# end class

class BottleNeckBlockSEFull(torch.nn.Module):
    def __init__(self, inchannels, interchannels, dropout, kernel=3, stride=1, downsample=None, activation="relu"):
        # inchannels = input channels
        # interchannels = internal channels
        # Source:
        # https://github.com/last-one/tools/blob/master/pytorch/SE-ResNeXt/SeResNeXt.py
        super(BottleNeckBlock, self).__init__()
        self.convblock1 = ConvABlock(inchannels=inchannels,
                                     outchannels=interchannels,
                                     kernel=1,
                                     stride=stride,
                                     dropout=dropout,
                                     activation=activation)
        self.convblock2 = ConvABlock(inchannels=interchannels,
                                     outchannels=interchannels,
                                     kernel=kernel,
                                     stride=stride,
                                     dropout=dropout,                                
                                     activation=activation)  
        self.convblock3 = ConvABlock(inchannels=interchannels,
                                     outchannels=interchannels*4,
                                     kernel=1,
                                     stride=stride,
                                     dropout=dropout,                                     
                                     activation=activation) 
        # SE
        self.global_pool = torch.nn.AdaptiveAvgPool2d(1)
        self.conv_down = torch.nn.Conv2d(in_channels=interchannels*4,
                                         out_channels=interchannels//4,
                                         kernel_size=1,
                                         bias=False)
        self.relu = torch.nn.ReLU(inplace=True)
        self.conv_up = torch.nn.Conv2d(in_channels=interchannels//4,
                                       out_channels=interchannels,
                                       kernel_size=1,
                                       bias=False)
        self.sig = torch.nn.Sigmoid()
        # Downsample
        self.downsample = downsample
        self.stride = stride
    # end def

    def forward(self, x):
        residual = x
        out = self.convblock1(x)
        out = self.convblock2(out)
        out = self.convblock3(out)

        out1 = self.global_pool(out)
        out1 = self.conv_down(out1)
        out1 = self.relu(out1)
        out1 = self.conv_up(out1)
        out1 = self.sig(out1)

        if self.downsample is not None:
            residual = self.downsample(x)
        # end if

        res = out1 * out + residual
        res = self.relu(res)
        return res
    # end def
# end class

class HSwish(torch.nn.Module):
    # https://github.com/arawxx/MobileNetV3/blob/main/
    def __init__(self):
        super(HSwish, self).__init__()
        self.relu6 = torch.nn.ReLU6(inplace=True)
    # end def

    def forward(self, x):
        x = x*self.relu6(x+3)/6
        return x
    # end def
# end def

class HSigmoid(torch.nn.Module):
    # https://github.com/arawxx/MobileNetV3/blob/main/
    def __init__(self):
        super(HSigmoid, self).__init__()
        self.relu6 = torch.nn.ReLU6(inplace=True)
    # end def

    def forward(self, x):
        x = self.relu6(x+3)/6
        return x
    # end def
# end def



class SELLayer(torch.nn.Module):
    def __init__(self, interchannels, r=16):
        super(SELLayer, self).__init__()
        
        if interchannels % r!=0:
            raise ValueError('inchannels must be divisible by reduction (default=16).')
        # end if
        self.line1 = torch.nn.Linear(in_features=interchannels,
                                     out_features=interchannels//r,
                                     bias=True)
        self.relu = torch.nn.ReLU(inplace=True)
        self.line2 = torch.nn.Linear(in_features=interchannels//r,
                                     out_features=interchannels,
                                     bias=True)
        self.sigmoid = torch.nn.Sigmoid()
    # end def

    def forward(self, x):
        out = F.avg_pool2d(x, kernel_size=x.size()[2:4])
        out = out.permute(0,2,3,1)
        out = self.relu(self.line1(out))
        out = self.sigmoid(self.line2(out))
        out = out.permute(0,3,1,2)

        return out*x
    # end def
# end class

class BottleNeckBlockSE(torch.nn.Module):
    def __init__(self, inchannels, interchannels, dropout, kernel=3, stride=1, activation="relu"):
        # inchannels = input channels
        # interchannels = internal channels
        # Source:
        # https://github.com/last-one/tools/blob/master/pytorch/SE-ResNeXt/SeResNeXt.py
        super(BottleNeckBlock, self).__init__()
        self.convblock1 = ConvABlock(inchannels=inchannels,
                                     outchannels=interchannels,
                                     kernel=1,
                                     dropout=dropout,
                                     stride=1,
                                     activation=activation)
        self.convblock2 = ConvABlock(inchannels=interchannels,
                                     outchannels=interchannels,
                                     kernel=kernel,
                                     stride=stride,
                                     dropout=dropout,                                
                                     activation=activation)  
        self.convblock3 = ConvABlock(inchannels=interchannels,
                                     outchannels=interchannels*4,
                                     kernel=1,
                                     stride=1,
                                     dropout=dropout,                                     
                                     activation=activation) 
        # SE
        self.selayer = SELayer(interchannels*4)
        downsample = None
        if stride!=1 or interchannels != inchannels*4:
            downsample = torch.nn.Sequential(torch.nn.Conv2d(interchannels,
                                                             inchannels*4,
                                                             kernel_size=1,
                                                             stride=stride,
                                                             bias=False),
                                            torch.nn.BatchNorm2d(inchannels*4))
        # end if
        self.downsample = downsample
        self.stride = stride
        self.relu = torch.nn.ReLU(inplace=True)
    # end def

    def forward(self, x):
        residual = x
        out = self.convblock1(x)
        out = self.convblock2(out)
        out = self.convblock3(out)

        out = self.selayer(out)

        if self.downsample is not None:
            residual = self.downsample(x)
        # end if

        out += residual
        out = self.relu(out)
        return out
    # end def
# end class

class InvertedBottleNeckBlock(torch.nn.Module):
    # This concept is from MobileNetV2!
    # inter = in_channels *4
    def __init__(self, inchannels, interchannels, dropout, kernel=3, stride=1, activation="relu"):
        super(InvertedBottleNeckBlock, self).__init__()
        # Expansion Convolution
        self.convblock1 = ConvABlock(inchannels=inchannels,
                                     outchannels=interchannels,
                                     kernel=1,
                                     stride=1,
                                     dropout=dropout,                                     
                                     activation=activation)
        # Depthwise Convolution
        self.convblock2 = ConvABlock(inchannels=interchannels,
                                     outchannels=interchannels,
                                     kernel=kernel,
                                     stride=stride,
                                     dropout=dropout,                                      
                                     activation=activation,
                                     groups=interchannels)  
        # Pointwise Convolution
        self.convlow = torch.nn.Conv2d(in_channels=interchannels,
                                       out_channels=inchannels,
                                       kernel_size=1,
                                       stride=1,
                                       bias=False)
        self.bnlow = torch.nn.BatchNorm2d(inchannels)                                  
    # end def

    def forward(self, x):
        out = self.convblock1(x)
        out = self.convblock2(out)
        out = self.convlow(out)
        out = self.bnlow(out)
        out += x
        return out
    # end def
# end class   

class ConvNeXtBlock(torch.nn.Module):
    # This concept is from MobileNetV2!
    def __init__(self, inchannels, interchannels):
        super(ConvNeXtBlock, self).__init__()
        self.convblock1 = torch.nn.Conv2d(in_channels=inchannels,
                                          out_channels=interchannels,
                                          kernel_size=7,
                                          stride=1,
                                          padding=3,
                                          bias=False)
                                        #   groups=interchannels,
        # self.bnblock1 = torch.nn.LayerNorm(interchannels) 
        self.bnblock1 = torch.nn.BatchNorm2d(interchannels)
        self.convblock2 = torch.nn.Conv2d(in_channels=interchannels,
                                          out_channels=interchannels,
                                          kernel_size=1,
                                          stride=1,
                                          bias=False) 
        self.actblock2 = torch.nn.GELU()
        self.convblock3 = torch.nn.Conv2d(in_channels=interchannels,
                                          out_channels=inchannels,
                                          kernel_size=1,
                                          stride=1,
                                          bias=False)                                     
    # end def

    def forward(self, x):
        # print('x: ', x.shape)
        out = self.convblock1(x)
        out = self.bnblock1(out)
        out = self.convblock2(out)
        out = self.actblock2(out)
        out = self.convblock3(out)
        # print('out: ', out.shape)
        out += x

        return out
    # end def
# end class 

class SELayer(torch.nn.Module):
    def __init__(self, interchannels, squeeze=4):
        super(SELayer, self).__init__()
        # https://github.com/arawxx/MobileNetV3/blob/main/
        # Sqeeeze and Excite Layer
        self.global_pool = torch.nn.AdaptiveAvgPool2d(1)
        self.conv1 = torch.nn.Conv2d(in_channels=interchannels,
                                     out_channels=interchannels//squeeze,
                                     kernel_size=1,
                                     stride=1,
                                     bias=False)
        self.bn1 = torch.nn.BatchNorm2d(interchannels//squeeze)
        self.relu = torch.nn.ReLU(inplace=True)
        self.conv2 = torch.nn.Conv2d(in_channels=interchannels//squeeze,
                                     out_channels=interchannels,
                                     kernel_size=1,
                                     stride=1,
                                     bias=False)
        self.bn2 = torch.nn.BatchNorm2d(interchannels)
        self.sigmoid = HSigmoid()
    # end def

    def forward(self, x):
        out = self.global_pool(x)
        out = self.conv1(out)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.sigmoid(out)

        return out*x
    # end def
# end class

class InvertedBottleNeckBlockSE(torch.nn.Module):
    # This concept is from MobileNetV2!
    # inter = in_channels *4
    def __init__(self, inchannels, interchannels, dropout, kernel=3, stride=1, SE=False, activation="relu"):
        super(InvertedBottleNeckBlockSE, self).__init__()
        # Expansion Convolution
        self.convblock1 = ConvABlock(inchannels=inchannels,
                                     outchannels=interchannels,
                                     kernel=1,
                                     stride=1,
                                     dropout=dropout,                                     
                                     activation=activation)
        # Depthwise Convolution
        self.convblock2 = ConvABlock(inchannels=interchannels,
                                     outchannels=interchannels,
                                     kernel=kernel,
                                     stride=stride,
                                     dropout=dropout,                                      
                                     activation=activation,
                                     groups=interchannels)  
        
        # SE Layer
        self.selayer = None if SE==False else SELayer(interchannels)
        
        # Pointwise Convolution
        self.convlow = torch.nn.Conv2d(in_channels=interchannels,
                                       out_channels=inchannels,
                                       kernel_size=1,
                                       stride=1,
                                       bias=False)
        self.bnlow = torch.nn.BatchNorm2d(inchannels) 

        self.downsample = None if stride == 1 else torch.nn.Sequential(torch.nn.Conv2d(inchannels, inchannels, kernel_size=1, stride=stride, bias=False),
                                                                       torch.nn.BatchNorm2d(inchannels))

    # end def

    def forward(self, x):
        residual = x
        out = self.convblock1(x)
        out = self.convblock2(out)
        if self.selayer:
            out = self.selayer(out)
        out = self.convlow(out)
        out = self.bnlow(out)
        if self.downsample:
            residual=self.downsample(x)
        out += residual
        return out
    # end def
# end class 

class MidInvBlock(torch.nn.Module):
    """
    Simple convolution block with choice of activation:
    (1) ReLU
    (2) GeLU
    (3) PReLU
    Stride 1 is often used when you want to maintain the
    spatial dimensions (height, width) of the feature maps
    """
    def __init__(self, inchannels, outchannels, kernel=3, stride=1, groups=1, padding=0, activation="relu"):
        activations = {
            "relu" : torch.nn.ReLU(inplace=True),
            "prelu": torch.nn.PReLU(num_parameters=outchannels),
            "relu6" : torch.nn.ReLU6(),
            "gelu": torch.nn.GELU(),
        }
        super(MidInvBlock, self).__init__()
            # bias=False : assuming bias is not learned and reduces learnable parameters
        self.convblock = torch.nn.Conv2d(in_channels=inchannels, 
                                         out_channels=outchannels, 
                                         kernel_size=kernel, 
                                         stride=stride, 
                                         padding=padding, 
                                         groups=groups, 
                                         bias=False)
        self.bnblock = torch.nn.BatchNorm2d(outchannels)
        self.activation = activations.get(activation, torch.nn.ReLU(inplace=True)) # True to save memory
        
    # end def

    def forward(self,x):
        # print('x mininv: ',x.shape)
        out = self.convblock(x)
        out = self.bnblock(out)
        out = self.activation(out)

        return out
    # end def

# end class

class LargeBlock1(torch.nn.Module):
    def __init__(self,inchannels,interchannels,block,dropout,SE=False,stride=1,activation="relu"):
        blocks = []
        super(LargeBlock1, self).__init__()
        if block == 'convnext':
            self.CB1 = ConvNeXtBlock(inchannels=inchannels,
                                     interchannels=inchannels*4)
            self.middle = MidInvBlock(inchannels=inchannels,
                                      outchannels=interchannels,
                                      padding=1,
                                      activation=activation)
            self.CB2 = ConvNeXtBlock(inchannels=interchannels,
                                     interchannels=interchannels*4)

        elif block == 'ibottleneck':
            self.CB1 = InvertedBottleNeckBlock(inchannels=inchannels,
                                               interchannels=inchannels*4,
                                               dropout=dropout,
                                               activation=activation)
            self.middle = MidInvBlock(inchannels=inchannels,
                                      outchannels=interchannels,
                                      padding=1,
                                      activation=activation)           
            self.CB2 = InvertedBottleNeckBlock(inchannels=interchannels,
                                               interchannels=interchannels*4,
                                               dropout=dropout,
                                               activation=activation)
    
        elif block == 'ibottleneckse':
            self.CB1 = InvertedBottleNeckBlockSE(inchannels=inchannels,
                                                 interchannels=inchannels*4,
                                                 dropout=dropout,
                                                 SE=SE,
                                                 stride=stride,
                                                 activation=activation)
            self.middle = MidInvBlock(inchannels=inchannels,
                                      outchannels=interchannels,
                                      padding=1,
                                      activation=activation) 
            self.CB2 = InvertedBottleNeckBlockSE(inchannels=interchannels,
                                                 interchannels=interchannels*4,
                                                 dropout=dropout,
                                                 SE=SE,
                                                 stride=stride,
                                                 activation=activation)          

        else:
            raise ValueError(f"Unsupported block type: {block}")
        # end if-elif

    # end def

    def forward(self,x):
        out = self.CB1(x)
        out = self.middle(out)
        out = self.CB2(out)

        return out
    # end def
# end class

           

class Network(torch.nn.Module):
    """
    The Very Low early deadline architecture is a 5-layer CNN. Keep in mind the parameter limit is 21M.
    """
    def __init__(self, in_feats=3, channels=[64,256,512], block='convnext', dropout=[0.1,0.1,0.1], SE=[False,True,True], strides=[1,2,2], activation='relu', num_classes=7001):
        super(Network, self).__init__()
        layers = []
        # in_feats=3
        # channels=64
        # channels2 = 256
        # # blockstype='convnext'
        # blockstype='ibottleneck'
        # activation='relu'
        # [3->input,64,96,128,192,256,512,1024]
        self.blocks = torch.nn.ModuleList()
        self.blocks.append(ConvABlock(inchannels=in_feats,
                                      outchannels=channels[0],
                                      kernel=7,
                                      stride=2,
                                      activation=activation))
        self.blocks.append(torch.nn.MaxPool2d(kernel_size=3,
                                              stride=2,
                                              padding=1))
        for i in range(len(channels)-1):
            self.blocks.append(LargeBlock1(inchannels=channels[i],
                                           interchannels=channels[i+1],
                                           block=block,
                                           dropout=dropout[i],
                                           SE=SE[i],
                                           stride=strides[i],
                                           activation=activation))
        # end for


        self.blocks.append(torch.nn.AdaptiveAvgPool2d((1,1)))
        self.blocks.append(torch.nn.Flatten())
        self.backbone = torch.nn.Sequential(*self.blocks)

        # self.cls_layer = torch.nn.Linear(channels[-1],num_classes)
        self.cls_layer = torch.nn.Linear(channels[-1],num_classes)

    def forward(self, x, return_feats=False):
        """
        What is return_feats? It essentially returns the second-to-last-layer
        features of a given image. It's a "feature encoding" of the input image,
        and you can use it for the verification task. You would use the outputs
        of the final classification layer for the classification task.

        You might also find that the classification outputs are sometimes better
        for verification too - try both.
        """
        feats = self.backbone(x)
        out = self.cls_layer(feats)

        if return_feats:
            return feats
        else:
            return out
# end class

def initialize_weights(tensor):
    if type(tensor) == torch.nn.Conv2d or type(tensor) == torch.nn.Linear:
        torch.nn.init.kaiming_normal_(tensor.weight.data)
    # end if
# end def

"""# Let's train!"""

def train(model, dataloader, optimizer, criterion, config, scheduler, scaler, epoch):

    model.train()

    # Progress Bar
    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)

    num_correct = 0
    total_loss  = 0

    iters = len(dataloader)

    for i, (images, labels) in enumerate(dataloader):

        optimizer.zero_grad() # Zero gradients

        images, labels = images.to(DEVICE), labels.to(DEVICE)

        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it!
            outputs = model(images)
            loss    = criterion(outputs, labels)

        # Update no. of correct predictions & loss as we iterate
        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())
        total_loss      += float(loss.item())

        # tqdm lets you add some details so you can monitor training as you train.
        batch_bar.set_postfix(
            acc         = "{:.04f}%".format(100 * num_correct / (config['batch_size']*(i + 1))),
            loss        = "{:.04f}".format(float(total_loss / (i + 1))),
            num_correct = num_correct,
            lr          = "{:.04f}".format(float(optimizer.param_groups[0]['lr']))
        )

        scaler.scale(loss).backward() # This is a replacement for loss.backward()
        scaler.step(optimizer) # This is a replacement for optimizer.step()
        scaler.update()

        # TODO? Depending on your choice of scheduler,
        # You may want to call some schdulers inside the train function. What are these?
        # scheduler.step()
        scheduler.step(epoch + i / iters)


        batch_bar.update() # Update tqdm bar

    batch_bar.close() # You need this to close the tqdm bar

    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))
    total_loss  = float(total_loss / len(dataloader))

    return acc, total_loss

def validate(model, dataloader, criterion, config):

    model.eval()
    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)

    num_correct = 0.0
    total_loss = 0.0

    for i, (images, labels) in enumerate(dataloader):

        # Move images to device
        images, labels = images.to(DEVICE), labels.to(DEVICE)

        # Get model outputs
        with torch.inference_mode():
            outputs = model(images)
            loss = criterion(outputs, labels)

        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())
        total_loss += float(loss.item())

        batch_bar.set_postfix(
            acc="{:.04f}%".format(100 * num_correct / (config['batch_size']*(i + 1))),
            loss="{:.04f}".format(float(total_loss / (i + 1))),
            num_correct=num_correct)

        batch_bar.update()

    batch_bar.close()
    acc = 100 * num_correct / (config['batch_size']* len(dataloader))
    total_loss = float(total_loss / len(dataloader))
    return acc, total_loss
# end def



"""# Experiments"""
def experiments(model, train_loader, valid_loader,optimizer, criterion, config, scheduler, scaler, run):
    patience = 30
    best_valacc = 0.0
    no_improvement_count = 0
    delta = 0.0001

    for epoch in range(config['epochs']):

        # curr_lr = float(optimizer.param_groups[0]['lr'])
        curr_lr = scheduler.get_lr()[0]
        train_acc, train_loss = train(model, train_loader, optimizer, criterion, config, scheduler, scaler, epoch)

        print("\nEpoch {}/{}: \nTrain Acc {:.04f}%\t Train Loss {:.04f}\t Learning Rate {:.04f}".format(
            epoch + 1,
            config['epochs'],
            train_acc,
            train_loss,
            curr_lr))

        val_acc, val_loss = validate(model, valid_loader, criterion, config)

        print("Val Acc {:.04f}%\t Val Loss {:.04f}".format(val_acc, val_loss))

        wandb.log({"train_loss":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc,
                'validation_loss': val_loss, "learning_Rate": curr_lr})

        # If you are using a scheduler in your train function within your iteration loop, you may want to log
        # your learning rate differently
        # scheduler.step(val_acc)
        # scheduler.step()


        # #Save model in drive location if val_acc is better than best recorded val_acc
        if val_acc >= (best_valacc+delta):
        #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')
            print("Saving model")
            torch.save({'model_state_dict':model.state_dict(),
                        'optimizer_state_dict':optimizer.state_dict(),
                        'scheduler_state_dict':scheduler.state_dict(),
                        'val_acc': val_acc,
                        'epoch': epoch}, './checkpoint_'+str(config['run'])+'.pth')
            
            best_valacc = val_acc
            wandb.save('checkpoint_'+str(config['run'])+'.pth')
            no_improvement_count = 0
        else:
            no_improvement_count+=1
        # You may find it interesting to exlplore Wandb Artifcats to version your models

        if no_improvement_count >=patience:
            print(f"Early stopping after {epoch+1} epochs due to no improvement in validation accuracy.")
            break            
    run.finish()
# end def

"""# Classification Task: Testing"""

def test(model,dataloader):

  model.eval()
  batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')
  test_results = []

  for i, (images) in enumerate(dataloader):
      # TODO: Finish predicting on the test set.
      images = images.to(DEVICE)

      with torch.inference_mode():
        outputs = model(images)

      outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()
      test_results.extend(outputs)

      batch_bar.update()

  batch_bar.close()
  return test_results



def eval_verification(unknown_images, known_images, model, similarity, batch_size, threshold, mode='val'):

    unknown_feats, known_feats = [], []

    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)
    model.eval()

    # We load the images as batches for memory optimization and avoiding CUDA OOM errors
    for i in range(0, unknown_images.shape[0], batch_size):
        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size

        with torch.no_grad():
            unknown_feat = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model
        unknown_feats.append(unknown_feat)
        batch_bar.update()

    batch_bar.close()

    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)

    for i in range(0, known_images.shape[0], batch_size):
        known_batch = known_images[i:i+batch_size]
        with torch.no_grad():
              known_feat = model(known_batch.float().to(DEVICE), return_feats=True)

        known_feats.append(known_feat)
        batch_bar.update()

    batch_bar.close()

    # Concatenate all the batches
    unknown_feats = torch.cat(unknown_feats, dim=0)
    known_feats = torch.cat(known_feats, dim=0)

    # similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])
    similarity_values = torch.stack([similarity(unknown_feats, known_feature.unsqueeze(0)) for known_feature in known_feats])
    # Print the inner list comprehension in a separate cell - what is really happening?

    max_similarity_values, predictions = similarity_values.max(0) #Why are we doing an max here, where are the return values?
    max_similarity_values, predictions = max_similarity_values.cpu().numpy(), predictions.cpu().numpy()


    # Note that in unknown identities, there are identities without correspondence in known identities.
    # Therefore, these identities should be not similar to all the known identities, i.e. max similarity will be below a certain
    # threshold compared with those identities with correspondence.

    # In early submission, you can ignore identities without correspondence, simply taking identity with max similarity value
    # pred_id_strings = [known_paths[i] for i in predictions] # Map argmax indices to identity strings

    # After early submission, remove the previous line and uncomment the following code

    NO_CORRESPONDENCE_LABEL = 'n000000'
    pred_id_strings = []
    for idx, prediction in enumerate(predictions):
        if max_similarity_values[idx] < threshold: # why < ? Thank about what is your similarity metric
            pred_id_strings.append(NO_CORRESPONDENCE_LABEL)
        else:
            pred_id_strings.append(known_paths[prediction])

    if mode == 'val':
      true_ids = pd.read_csv('/mnt/nvme/home/bbabatun/IDL/HW2P2/data/11-785-f23-hw2p2-verification/verification_dev.csv')['label'].tolist()
      accuracy = accuracy_score(pred_id_strings, true_ids)
      print("Verification Accuracy = {}".format(accuracy))

    return pred_id_strings
# end def

def get_args():
    parser = argparse.ArgumentParser(description="Parameters")

    # Add command-line arguments for each configuration parameter
    parser.add_argument("--batch_size", type=int, default=128, help="Batch size")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--epochs", type=int, default=478, help="Number of epochs")
    parser.add_argument("--channels", type=int, nargs="+", default=[32, 64, 128, 256,512], help="List of channels")
    parser.add_argument("--strides", type=int, nargs="+", default=[1, 1, 2, 2, 2], help="List of strides")
    parser.add_argument("--blocks", type=str, default='ibottleneckse', help="Type of blocks used")
    parser.add_argument("--activation", type=str, default='relu6', help="Activation used")
    parser.add_argument("--run", type=int, default=16, help="Run number")
    parser.add_argument("--threshold", type=float, default=0.4, help="Verification threshold")
    parser.add_argument("--dropout", type=float, nargs="+", default=[0.2,0.0,0.2,0.0,0.2], help="Dropoutrate")  
    parser.add_argument("--SE", type=float, nargs="+", default=[True,True,True,True,True], help="Squeeze Excitation") 

    args = parser.parse_args()
    return args


if __name__ == "__main__":
    torch.cuda.empty_cache()
    gc.collect()
    # python3 ~/IDL/HW2P2/hw2p2.py --batch_size 128 --lr 0.001 --epochs 100 --channels 64 128 256 --blocks convnext --depths 1 1 1 --activation relu6 --run 0 --threshold 0.4
    # config = {
    #     'batch_size': 64, # Increase this if your GPU can handle it
    #     'lr': 1e-3,
    #     'epochs': 20, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.
    #     'channels' : [3,64,128,256,512,1024],
    #     'kernels' : [7,3,3,3,3],
    #     'stride' : [4,2,2,2,2]
    #     # Include other parameters as needed.
    # }
    args = get_args()
    print(args)
    config = {
        'batch_size': args.batch_size, # Increase this if your GPU can handle it
        'lr': args.lr,
        'epochs': args.epochs, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.
        'channels' : args.channels,
        'blocks' : args.blocks,
        'activation' : args.activation,
        'run' : args.run,
        'threshold' : args.threshold,
        'dropout' : args.dropout,
        'SE': args.SE,
        'strides': args.strides,
        'criterion' : 'CrossEntropyLoss',
        'optimizer' : 'SGD',
        'scheduler' : 'CosineAnnealingWarmRestarts',
        'scheduler_t': 20
        # Include other parameters as needed.
    }

    """# Classification Dataset"""
    DATA_DIR    = '/mnt/nvme/home/bbabatun/IDL/HW2P2/data/11-785-f23-hw2p2-classification/'# TODO: Path where you have downloaded the data
    TRAIN_DIR   = os.path.join(DATA_DIR, "train")
    VAL_DIR     = os.path.join(DATA_DIR, "dev")
    TEST_DIR    = os.path.join(DATA_DIR, "test")

    # Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html
    # Calculate the mean and standard deviation of the images
    # total_mean = np.zeros(3) # (R, G, B)
    # total_std = np.zeros(3) # (R, G, B)
    # MEAN = []
    # STD = []
    # count = 0

    # for root,dirs,files in os.walk(TRAIN_DIR):
    #   for file in files:
    #     img = Image.open(os.path.join(root, file))
    #     img = np.array(img)

    #     total_mean += np.mean(img, axis=(0,1))
    #     total_std += np.std(img, axis=(0,1))
    #     count+=1
    #   # end for
    #   MEAN.append(total_mean/count)
    #   STD.append(total_std / count)
    #   count = 0
    # # end for

    # mean = total_mean / count
    # std = total_std / count
    # print(mean, std)

    mean = np.array([130.11541008, 102.36650074, 89.46795437])/255.0
    std = np.array([69.0567937, 60.24833864, 56.76284377])/255.0
    # mean=np.array([0.485, 0.456, 0.406])
    # std=np.array([0.229, 0.224, 0.225])
    # count = 140020

    train_transforms = torchvision.transforms.Compose([
        # transform an image to a PyTorch tensor
        # scales pixel values between 0.0 and 1.0
        # torchvision.transforms.ColorJitter(brightness=0.5, hue=0.3),
        # torchvision.transforms.RandomPerspective(distortion_scale=0.6, p=1.0),
        torchvision.transforms.RandomHorizontalFlip(),
        # normalizing images means transforming images where
        # mean and stddev of the image become 0.0 and 1.0 respectively
        # torchdivision.tranforms.Normalize(mean,std,inplace)
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean, std)

        ])# Implementing the right train transforms/augmentation methods is key to improving performance.

    # Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()
    # But there are some transforms which are performed after ToTensor() : e.g - Normalization
    # Normalization Tip - Do not blindly use normalization that is not suitable for this dataset

    valid_transforms = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean, std)
    ])


    train_dataset   = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms)
    valid_dataset   = torchvision.datasets.ImageFolder(VAL_DIR, transform= valid_transforms)
    # You should NOT have data augmentation on the validation set. Why?

    # Create data loaders
    train_loader = torch.utils.data.DataLoader(
        dataset     = train_dataset,
        batch_size  = config['batch_size'],
        shuffle     = True,
        num_workers = 3,
        pin_memory  = True
    )

    valid_loader = torch.utils.data.DataLoader(
        dataset     = valid_dataset,
        batch_size  = config['batch_size'],
        shuffle     = False,
        num_workers = 2
    )

    test_dataset = ClassificationTestDataset(TEST_DIR, transforms = valid_transforms) #Why are we using val_transforms for Test Data?
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = False,
                            drop_last = False, num_workers = 2)

    print("Number of classes    : ", len(train_dataset.classes))
    print("No. of train images  : ", train_dataset.__len__())
    print("Shape of image       : ", train_dataset[0][0].shape)
    print("Batch size           : ", config['batch_size'])
    print("Train batches        : ", train_loader.__len__())
    print("Val batches          : ", valid_loader.__len__())
    DataVisualize(TRAIN_DIR, train_transforms, config)

    torch.cuda.empty_cache()
    gc.collect()

    # depths=config['depths'], 
    model = Network(channels=config['channels'], block=config['blocks'], SE=config['SE'], dropout=config['dropout'], strides=config['strides'], activation=config['activation']).to(DEVICE)
    model.apply(initialize_weights)
    total_params = sum(p.numel() for p in model.parameters())
    print(total_params)
    max_allowed_params = 21_000_000
    if total_params > max_allowed_params:
        raise ValueError(f"Total params ({total_params:,}) exceed the maximum allowed ({max_allowed_params:,}). Exiting.")
    # end if

    """# Setup everything for training"""

    criterion = torch.nn.CrossEntropyLoss()
    # optimizer = torch.optim.AdamW(model.parameters(), lr= config['lr'], weight_decay= config['weight_decay'])
    # optimizer = torch.optim.Adam()
    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.99, weight_decay=5e-4, nesterov=True)
    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,
    #                                                     mode=config['scheduler_mode'],
    #                                                     factor=config['scheduler_factor'],
    #                                                     patience=config['scheduler_patience'],
    #                                                     threshold=config['scheduler_threshold'],
    #                                                     verbose=True)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,config['scheduler_t'])

    # You can try ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.
    scaler = torch.cuda.amp.GradScaler() # Good news. We have FP16 (Mixed precision training) implemented for you
    # It is useful only in the case of compatible GPUs such as T4/V100
    torch.cuda.empty_cache()
    gc.collect()


    """# Wandb"""
    wandb.login()

    # Create your wandb run
    run = wandb.init(
        name = "submission_"+str(config['run']), ## Wandb creates random run names if you skip this field
        reinit = True, ### Allows reinitalizing runs when you re-run this cell
        # run_id = ### Insert specific run id here if you want to resume a previous run
        # resume = "must" ### You need this to resume previous runs, but comment out reinit = True when using this
        project = "hw2p2-ablations", ### Project should be created in your wandb account
        config = config ### Wandb Config for your run
    )
    experiments(model, train_loader, valid_loader,optimizer, criterion, config, scheduler, scaler, run)
    test_results = test(model, test_loader)

    """## Generate csv to submit to Kaggle"""

    with open("classification_early_submission_"+str(config['run'])+".csv", "w+") as f:
        f.write("id,label\n")
        for i in range(len(test_dataset)):
            f.write("{},{}\n".format(str(i).zfill(6) + ".jpg", test_results[i]))

    command = "kaggle competitions submit -c 11-785-f23-hw2p2-classification -f classification_early_submission_"+str(config['run'])+".csv -m 'early submission'"
    subprocess.run(command, shell=True)

    """# Verification Task: Validation

    The verification task consists of the following generalized scenario:
    - You are given X unknown identitites
    - You are given Y known identitites
    - Your goal is to match X unknown identities to Y known identities.

    We have given you a verification dataset, that consists of 960 known identities, and 1080 unknown identities. The 1080 unknown identities are split into dev (360) and test (720). Your goal is to compare the unknown identities to the 1080 known identities and assign an identity to each image from the set of unknown identities. Some unknown identities do not have correspondence in known identities, you also need to identify these and label them with a special label n000000.

    Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities.

    This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification.
    """

    # This obtains the list of known identities from the known folder
    known_regex = "/mnt/nvme/home/bbabatun/IDL/HW2P2/data/11-785-f23-hw2p2-verification/known/*/*"
    known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]

    # Obtain a list of images from unknown folders
    unknown_dev_regex = "/mnt/nvme/home/bbabatun/IDL/HW2P2/data/11-785-f23-hw2p2-verification/unknown_dev/*"
    unknown_test_regex = "/mnt/nvme/home/bbabatun/IDL/HW2P2/data/11-785-f23-hw2p2-verification/unknown_test/*"

    # We load the images from known and unknown folders
    unknown_dev_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_dev_regex)))]
    unknown_test_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_test_regex)))]
    known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]

    # Why do you need only ToTensor() here?
    transforms = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean,std)])

    unknown_dev_images = torch.stack([transforms(x) for x in unknown_dev_images])
    unknown_test_images = torch.stack([transforms(x) for x in unknown_test_images])
    known_images  = torch.stack([transforms(y) for y in known_images ])
    #Print your shapes here to understand what we have done

    # You can use other similarity metrics like Euclidean Distance if you wish
    similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6)

    # verification eval
    pred_id_strings = eval_verification(unknown_dev_images, known_images, model, similarity_metric, config['batch_size'], config['threshold'], mode='val')
    # verification test
    pred_id_strings = eval_verification(unknown_test_images, known_images, model, similarity_metric, config['batch_size'], config['threshold'], mode='test')

    # add your finetune/retrain code here

    """## Generate csv to submit to Kaggle"""

    with open("verification_early_submission_"+str(config['run'])+".csv", "w+") as f:
        f.write("id,label\n")
        for i in range(len(pred_id_strings)):
            f.write("{},{}\n".format(i, pred_id_strings[i]))

    command = "kaggle competitions submit -c 11-785-f23-hw2p2-verification -f verification_early_submission_"+str(config['run'])+".csv -m 'early submission'"
    subprocess.run(command, shell=True)
# end def

"""    
    The first Conv layer has 64 channels, kernel size 7, and stride 4.
    The next three have 128, 256, 512 and 1024 channels. Each have kernel size 3 and stride 2.

    Think about strided convolutions from the lecture, as convolutioin with stride= 1 and downsampling.
    For stride 1 convolution, what padding do you need for preserving the spatial resolution?
    (Hint => padding = kernel_size // 2) - Why?)

    Why does a very simple network have 4 convolutions?
    Input images are 224x224. Note that each of these convolutions downsample.
    Downsampling 2x effectively doubles the receptive field, increasing the spatial
    region each pixel extracts features from. Downsampling 32x is standard
    for most image models.

    Why does a very simple network have high channel sizes?
    Every time you downsample 2x, you do 4x less computation (at same channel size).
    To maintain the same level of computation, you 2x increase # of channels, which
    increases computation by 4x. So, balances out to same computation.
    Another intuition is - as you downsample, you lose spatial information. We want
    to preserve some of it in the channel dimension.
    
"""

