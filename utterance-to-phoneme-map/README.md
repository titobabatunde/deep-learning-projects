# Intro to Deep Learning 11-685 HW3P2
Click [here](https://www.kaggle.com/competitions/automatic-speech-recognition-asr) for kaggle link.
<details open>
<summary> Information about Homework <i> (click on triangle to open) </i> </summary>

## Problem Statement
train model to derive transcriptions for the recordings

## Data
* The data includes a list of 40 phonemes from English
* The training and validation data composes of:
    * sequences of feature vectores derived from speech recordings
    * sequence of phonemes representing the transcription for each utterance
    * transcriptions are not time-aligned to the speech vectors
* The test data only composes of speech recordings
* **Goal**: train model to derive transcriptions for the recordings
    * "Yes" -> NN -> /Y/ /EH/ /S/
    * NN -> RNN +       dynamic programming (DP) algorithm (CTC) <br />
                            (Connectionist Temporal Classification)  

## Overview
* In both homeworks, the speech recordings are parametrized as a sequence of 
  <br /> feature vectors (mel spectral vectors, each respresenting a frame of speech)
  <br /> which arrive at a rate of 100 frames per second
* In HW1 the classification of one frame was not dependent by any other frame.
* HW1 algorithm also didn't consider where one phoneme ended and where the next
  <br /> began
    * while the output /Y/ /EH/ /S/ is order-aligned meaning sequence is in 
      <br /> correct order, it isn't time-synchronous (1-to-1 correspondence)
    * it also isn't apparent when the symbols must be output
    * Goal: output an order-aligned time-asynchronous label sequence that best
            <br /> respresents the input

## Alignment Problem of Inputs and Outputs
* We decompose the inference into a two-step process:
    * (1) -> The NN generates outputs at every time step
    * (2) -> Perform a dynamic programming search-like operation
             <br /> &ensp;&ensp;&ensp;&ensp;&ensp; on complete set of outputs generated by network, to 
             <br /> &ensp;&ensp;&ensp;&ensp;&ensp; generate the actual final output.
    * To note, the NN's acutal output is a vector of probabilities
      <br /> for each of the symbols (classes) in our vocabulary
    * A critical component is the "blank" symbol, which represents
      <br /> an invisible state that in removed from the final output
      <br /> sequence. This is important for repetitions.

## Problem Specifics
* Transcribing (converting) each speech recording into a sequence of
  <br /> phonemes from the list below:
  <br /> ["[SIL]", "NG", "F", "M", "AE", "R", "UW", "N", "IY", "AW", "V", "UH",  <br /> "OW", "AA", "ER", "HH", "Z", "K", "CH", "W", "EY", "ZH", "T", "EH", "Y", <br /> "AH", "B", "P", "TH", "DH", "AO", "G", "L", "JH", "OY", "SH", "D", "AY", <br /> "S", "IH"]
* This list consists of 39 actual phonemes and the ["SIL"] silence symbol
* List doesn't include the BLANK symbol (you need to include)-> 42 symbols
  <br /> outputs
* The list needs to be alphabetized and made into a dictionary
* After rearranging the phonemes, introduce BLANK at top of list

## Neural Network
* This must be a recurrent network (RNN)
    * long-term memory is best retained by LSTMs (or structures like GRUs)
* Assume entire speech recodring is available before recognition
    * so it can be bidirectional
* Although it was stated that symbol probabilities are computed for
  <br /> every input vector, this may not be required since the asynchronous
  <br /> output symbol sequence typically has far fewer symbols than the number 
  <br /> of vectors in the input. 
    * we could potentially downsample the input sequence within the nework
* Model taked in input sequence of vectores X(0),...,X(N-1) to produce
  <br /> probability vector
    * input sequence of speec vectors shows both short-term structural
      <br /> relations and long-term contextual dependence
    * 1D CNN can capture structural dependence between adjacent vectors
      <br /> and can have stride>1 (usually 2) to reduce the feature rate
    * Bidirectional LSTM can capture long-term contextual dependencies
    * pyramidal Bi-LSTMS / pBLSTMs reduce time resolution by a factor
      <br /> of 2 if stride=2
* pBLSTMs architecture in order:
    * encoder, which converts sequence of inputs x0,..,xN into a sequence
      <br /> of latent embeddings E_0,...,E_N (RNN Encoder). This is portion of
      <br /> network until final output layer (penultimate layer)
        * E = Encoder(X):
            <br /> &ensp; 01 = 1DCNN(X, stride=1, CNNparams) # could be ResNet
            <br /> &ensp; 02 = BiLSTM(01, BLSTMwidth, BLSTMparams)
            <br /> &ensp; E = pBLSTM(02, pBLSTMwidth, pBLSMparams) # reduces length by 2
            <br /> &ensp; *could add more pBLSTM layers to further fownsample*
    * embeddings passed to output layer to generate probability vector 
      at each time
      * LogP = NetworkComputeLogProbabilityTable(X)
            <br /> &ensp; E = Encoder(X)
            <br /> &ensp; Z = linear(E) # X is (batchsize, length, nsymbol)
            <br /> &ensp; LogP = LogSoftmax(Z)

## Training the model
* Each training instance: (x0,x1,...xN, /PH1/,...,/PHK/)
    * x0,x1,...,xN is the sequence of feature vectors from speech signal
    * /PH1/,...,/PHK/ is the phoneme sequence for the recording
* To compute the loss, we must expand our label sequence to align to the 
  <br /> input:
    * if we have 12 feature vectors x0,..,x11
    * the given label sequence /HH/ /AY/
    * we downsample by 2 to get 6 time-sychronous prob vectors P0,...,P5
    * the label sequence must be expanded to length 6, such as:
        <br /> /HH/ /HH/ /HH/ - /AY/ /AY/
        <br /> to match the output sequence of probability vectors from the NN
    * we can either use Viterbi algorithm to consider the most probable
      <br /> or consider all of them to train the network
          <br /> X (length, dim), Y (labellength)
        * L = Viterbi(X, Y)
            <br /> &ensp; LogP = NetworkComputeLogProbabilityTable(X)
            <br /> &ensp; Target = ViterviAlign(LogP, Y)
            <br /> &ensp; L = XentLoss(LogP, Target)
        
        * We compute the loss against every one of these possible 
          <br /> alignments. The overall loss is a weighted sum of all 
          <br /> these losses
          <br /> X (batchsize, length, dim), Y (batchsize, labellength)
        * L = NetworkCTCLoss(X, Y)
            <br /> &ensp; LogP = NetworkComputeLogProbabilityTable(X)
            <br /> &ensp; L = CTCLoss(LogP, Y) # forward-bakward algorithm
        * Then perform backward pass using L.backward()

## Inference
* Once we have a model that produces a vector of probabilities for every
  <br /> time step, how do we best convert this output table into a sequence

  * Greedy Search:
    * go through table and argmax algong the phoneme probabilities
      <br /> at each time
    * compresses the output squeezing out repetitions and removing
      <br /> blanks
    * X (length, dim) minibatch of input sequences
    * O = GreedyDecode(X, symbols)
        <br /> &ensp; P = NetworkComputeProbabilityTable(X)
        <br /> &ensp; A = ArgMax(P, dim=1)
        <br /> &ensp; O = []
        <br /> &ensp; for i=0:length-1
        <br /> &ensp;&ensp; if i == 0 || (A[i] != A[i-1] && symbols[A[i]] != BLANK)
        <br /> &ensp;&ensp;&ensp; O.attach(symbols[A[i]])
        <br /> &ensp;&ensp; end
        <br /> &ensp; end
    
    * Beam Search:
      * limit the scope of your exhaustive exponential search by
        <br /> an upper bound of k beams or sequences
      * while there is a real chance of missing the absolute best path 
        <br /> using this strategy, it is more sub-optimal than Greedy
      * use ctcdecode library in python: (width 3 for validation, may 
      <br />increase width for test data)

* Because sequences in mini-batch may all be of different length, we 
  <br /> pad the short sequences with 0-value, which carries no useful info
    * we must mask the meaningless regions using binary mask so that
      <br /> the zeros have no influence on computation

## Dataset
* mel-spectrograms that have 28 band frequencies for each time step 
    <br /> (Tin, 28), Tin is # of frames in utterance and Tout is the length
    <br /> of the output sequence per utterance
* a look up that maps each phoneme to a single character "phonetics.py"

## Kaggle Cuttoffs
1. Classification:
    * HIGH - 6 
    * MEDIUM - 8
    * LOW - 10

</details>

<br />
<br />

## Experimental Details
Click [here](https://wandb.ai/11685-cmu/hw3p2-ablations?workspace=user-titobabatunde) for wandb link <br />
1. Different architectures from start to finish with final validation distance of 4.58 <br /> before 60% of data was added. 

2. Shared Parameters:
    * Batch size of 64, CTCLoss, masking rate of 20%, and AdamW optimizer
    * All encoders use an expansive convolutional embedding (exapands to 2*encoder_hidden_size) followed by a single bidirectional LSTM and a single pyramidal bidirectional LSTM
    * The main components perturbed are the decoder layers, beam width, learning rate, and epoch size (see table below)
    * All use lr_scheduler.ReduceLROnPlateau but first two runs have factor=0.7 and patience=2, while the rest have factor = 0.5, patience=2, and threshold=1e-3

| Hidden Layers | Encoder Hidden Size | Beam Width | Test Beam Width | Learning Rate | Epoch | Valid distance | style="width:290px">Comments</div> | 
| :---------------- | :------: | :----: | :------: | :----: | :----: | :----: | :----: |
| [512,128] | 64 | 5 | 20 | 0.002 | 50 |  6.49 | Noice, for the very first time close to high cut off |
| [512,128] | 64 | 5 | 10 | 0.002 | 50 |  10.158 | Honestly not sure what I did here to make the distance drop but now stuck due to lost first file, curious to see the effects of lowering beam width |
| [512,128] | 64 | 2 | 20 | 0.001 | 100 |  7.642 | Okay we're now seeing a drop but I think it's only from learning rate drop and not beam width drop so lets increase that |
| [512,128] | 64 | 10 | 20 | 0.001 | 100 |  6.367 | Okay so this is clearly from beam wdith so lets ramp that up |
| [512,128] | 64 | 30 | 20 | 0.002 | 100 |  9.581 | Clearly beam width is too wide because of increase in val_dist |
| [512,128] | 64 | 15 | 20 | 0.001 | 150 |  6.315 | Okay we're now dropping beam width and we see a decrease in distance. Not sure what else to do other than try to increase width of hidden layers |
| [1024,1024] | 64 | 15 | 20 | 0.001 | 150 |  5.636 | Okay nice that does it |
| [2048,2048] | 64 | 15 | 20 | 0.001 | 150 |  4.588 | Increase further to see and passed high cutt off? |


## Running Instructions
python3 main.py
