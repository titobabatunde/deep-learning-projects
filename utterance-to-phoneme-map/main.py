# -*- coding: utf-8 -*-
"""IDL_F23_HW3_Starter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rdKhV_6VAIvSVQCd68vSSRqpW-CBtoIy

# Installs
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -q

"""
Data
-----
* The data includes a list of 40 phonemes from English

* The training and validation data composes of:
    * sequences of feature vectores derived from speech recordings
    * sequence of phonemes representing the transcription for each utterance
    * transcriptions are not time-aligned to the speech vectors
* The test data only composes of speech recordings
* Goal: train model to derive transcriptions for the recordings
            "Yes" -> NN -> /Y/ /EH/ /S/
            NN -> RNN +       dynamic programming (DP) algorithm (CTC)
                              (Connectionist Temporal Classification)

Overview
--------
* In both homeworks, the speech recordings are parametrized as a sequence of
  feature vectors (mel spectral vectors, each respresenting a frame of speech)
  which arrive at a rate of 100 frames per second
* In HW1 the classification of one frame was not dependent by any other frame.
* HW1 algorithm also didn't consider where one phoneme ended and where the next
  began
    * while the output /Y/ /EH/ /S/ is order-aligned meaning sequence is in 
      correct order, it isn't time-synchronous (1-to-1 correspondence)
    * it also isn't apparent when the symbols must be output
    * Goal: output an order-aligned time-asynchronous label sequence that best
            respresents the input

An Alignment Problem of Inputs and Outputs
------------------------------------------
* We decompose the inference into a two-step process:
    * (1) -> The NN generates outputs at every time step
    * (2) -> Perform a dynamic programming search-like operation
             on complete set of outputs generated by network, to 
             generate the actual final output.
    * To note, the NN's acutal output is a vector of probabilities
      for each of the symbols (classes) in our vocabulary
    * A critical component is the "blank" symbol, which represents
      an invisible state that in removed from the final output
      sequence. This is important for repetitions.

Problem Specifics
-----------------
* Transcribing (converting) each speech recording into a sequence of
  phonemes from the list below:
    ["[SIL]", "NG", "F", "M", "AE", "R", "UW", "N", "IY", "AW", "V", "UH", "OW", "AA",
    "ER", "HH", "Z", "K", "CH", "W", "EY", "ZH", "T", "EH", "Y", "AH", "B", "P", "TH",
    "DH", "AO", "G", "L", "JH", "OY", "SH", "D", "AY", "S", "IH"
    * This list consists of 39 actual phonemes and the [SIL] silence symbol
    * List doesn't include the BLANK symbol (you need to include)-> 42 symbols/outputs
    * The list needs to be alphabetized and made into a dictionary
    * After rearranging the phonemes, introduce BLANK at top of list

The Neural Network
------------------
* This must be a recurrent network (RNN)
    * long-term memory is best retained by LSTMs (or structures like GRUs)
* Assume entire speech recodring is available before recognition
    * so it can be bidirectional
* Although it was stated that symbol probabilities are computed for
  every input vector, this may not be required since the asynchronous
  output symbol sequence typically has far fewer symbols than the number 
  of vectors in the input. 
    * we could potentially downsample the input sequence within the nework
* Model taked in input sequence of vectores X(0),...,X(N-1) to produce
  probability vector
    * input sequence of speec vectors shows both short-term structural
      relations and long-term contextual dependence
    * 1D CNN can capture structural dependence between adjacent vectors
      and can have stride>1 (usually 2) to reduce the feature rate
    * Bidirectional LSTM can capture long-term contextual dependencies
    * pyramidal Bi-LSTMS / pBLSTMs reduce time resolution by a factor
      of 2 if stride=2
* pBLSTMs architecture in order:
    * encoder, which converts sequence of inputs x0,..,xN into a sequence
      of latent embeddings E_0,...,E_N (RNN Encoder). This is portion of
      network until final output layer (penultimate layer)
        * E = Encoder(X):
            01 = 1DCNN(X, stride=1, CNNparams) # could be ResNet
            02 = BiLSTM(01, BLSTMwidth, BLSTMparams)
            E = pBLSTM(02, pBLSTMwidth, pBLSMparams) # reduces length by 2
            # could add more pBLSTM layers to further fownsample
    * embeddings passed to output layer to generate probability vector 
      at each time
      * LogP = NetworkComputeLogProbabilityTable(X)
            E = Encoder(X)
            Z = linear(E) # X is (batchsize, length, nsymbol)
            LogP = LogSoftmax(Z)

Training the network
--------------------
* Each training instance: (x0,x1,...xN, /PH1/,...,/PHK/)
    * x0,x1,...,xN is the sequence of feature vectors from speech signal
    * /PH1/,...,/PHK/ is the phoneme sequence for the recording
* To compute the loss, we must expand our label sequence to align to the 
  input:
    * if we have 12 feature vectors x0,..,x11
    * the given label sequence /HH/ /AY/
    * we downsample by 2 to get 6 time-sychronous prob vectors P0,...,P5
    * the label sequence must be expanded to length 6, such as:
        /HH/ /HH/ /HH/ - /AY/ /AY/
        to match the output sequence of probability vectors from the NN
    * we can either use Viterbi algorithm to consider the most probable
      or consider all of them to train the network
          X (length, dim), Y (labellength)
        * L = Viterbi(X, Y)
            LogP = NetworkComputeLogProbabilityTable(X)
            Target = ViterviAlign(LogP, Y)
            L = XentLoss(LogP, Target)
        
        * We compute the loss against every one of these possible 
          alignments. The overall loss is a weighted sum of all 
          these losses
          X (batchsize, length, dim), Y (batchsize, labellength)
        * L = NetworkCTCLoss(X, Y)
            LogP = NetworkComputeLogProbabilityTable(X)
            L = CTCLoss(LogP, Y) # forward-bakward algorithm
        * Then perform backward pass using L.backward()

Inference
---------
* Once we have a model that produces a vector of probabilities for every
  time step, how do we best convert this output table into a sequence

  * Greedy Search:
    * go through table and argmax algong the phoneme probabilities
      at each time
    * compresses the output squeezing out repetitions and removing
      blanks
    * X (length, dim) minibatch of input sequences
    * O = GreedyDecode(X, symbols)
        P = NetworkComputeProbabilityTable(X)
        A = ArgMax(P, dim=1)
        O = []
        for i=0:length-1
            if i == 0 || (A[i] != A[i-1] && symbols[A[i]] != BLANK)
                O.attach(symbols[A[i]])
            end
        end
    
    * Beam Search:
      * limit the scope of your exhaustive exponential search by
        an upper bound of k beams or sequences
      * while there is a real chance of missing the absolute best path 
        using this strategy, it is more sub-optimal than Greedy
      * use ctcdecode library in python
            * width 3 for validation
            * may increase width for test data

* Because sequences in mini-batch may all be of different length, we 
  pad the short sequences with 0-value, which carries no useful info
    * we must mask the meaningless regions using binary mask so that
      the zeros have no influence on computation

Dataset
-------
* mel-spectrograms that have 28 band frequencies for each time step 
    (Tin, 28), Tin is # of frames in utterance and Tout is the length
    of the output sequence per utterance
* a look up that maps each phoneme to a single character "phonetics.py"


"""
"""# Imports"""

import torch
import random
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torchsummaryX import summary
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
from torch.nn.utils.rnn import PackedSequence
from torch.autograd import Variable

import torchaudio.transforms as tat

from sklearn.metrics import accuracy_score
import gc

import zipfile
import pandas as pd
from tqdm import tqdm
import os
import datetime
import subprocess

# imports for decoding and distance calculation
import ctcdecode
import Levenshtein
from ctcdecode import CTCBeamDecoder

import warnings
warnings.filterwarnings('ignore')

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Device: ", device)

"""# Dataset and Dataloader"""

# ARPABET PHONEME MAPPING
# DO NOT CHANGE

CMUdict_ARPAbet = {
    "" : " ",
    "[SIL]": "-", "NG": "G", "F" : "f", "M" : "m", "AE": "@",
    "R"    : "r", "UW": "u", "N" : "n", "IY": "i", "AW": "W",
    "V"    : "v", "UH": "U", "OW": "o", "AA": "a", "ER": "R",
    "HH"   : "h", "Z" : "z", "K" : "k", "CH": "C", "W" : "w",
    "EY"   : "e", "ZH": "Z", "T" : "t", "EH": "E", "Y" : "y",
    "AH"   : "A", "B" : "b", "P" : "p", "TH": "T", "DH": "D",
    "AO"   : "c", "G" : "g", "L" : "l", "JH": "j", "OY": "O",
    "SH"   : "S", "D" : "d", "AY": "Y", "S" : "s", "IH": "I",
    "[SOS]": "[SOS]", "[EOS]": "[EOS]"
}

CMUdict = list(CMUdict_ARPAbet.keys())
ARPAbet = list(CMUdict_ARPAbet.values())
 # Remove [SOS] and [EOS] from the transcript (Is there an efficient way to do this
            # without traversing through the transcript)

PHONEMES = CMUdict[:-2]
LABELS = ARPAbet[:-2]

# You might want to play around with the mapping as a sanity check here

"""### Train Data"""

class AudioDataset(torch.utils.data.Dataset):

    # For this homework, we give you full flexibility to design your data set class.
    # Hint: The data from HW1 is very similar to this HW

    def __init__(self, root, phonemes = PHONEMES, partition="train-clean-100"):
        '''
        Initializes the dataset.

        * The training and validation data composes of:
            * sequences of feature vectores derived from speech recordings
            * sequence of phonemes representing the transcription for each utterance
            * transcriptions are not time-aligned to the speech vectors

        INPUTS: What inputs do you need here?
        '''

        # Load the directory and all files in them

        self.mfcc_dir = os.path.join(root, partition, 'mfcc')
        self.transcript_dir = os.path.join(root, partition, 'transcript')

        self.mfcc_files = sorted(os.listdir(self.mfcc_dir)) # X
        self.transcript_files = sorted(os.listdir(self.transcript_dir)) # Y

        self.phonemes = phonemes
        # Making sure that we have the same no. of mfcc and transcripts
        assert len(self.mfcc_files) == len(self.transcript_files)

        self.mfccs, self.transcripts = [], []

        # Iterate through mfccs and transcripts
        for i in range(len(self.mfcc_files)):
        #   Load a single mfcc
            mfcc        = np.load(os.path.join(self.mfcc_dir, self.mfcc_files[i]))
        #   Do Cepstral Normalization of mfcc (explained in writeup)
            mfcc -= np.mean(mfcc, axis=0)
            mfcc /= np.std(mfcc, axis=0)
            #   Load the corresponding transcript
            # Remove [SOS] and [EOS] from the transcript
            transcript  = np.load(os.path.join(self.transcript_dir, self.transcript_files[i]))[1:-1] 
            # (Is there an efficient way to do this without traversing through the transcript?)
            # Note that SOS will always be in the starting and EOS at end, as the name suggests.
        #   Append each mfcc to self.mfcc, transcript to self.transcript
            self.mfccs.append(mfcc)
            self.transcripts.append(transcript)

        # WHAT SHOULD THE LENGTH OF THE DATASET BE?
        self.length = len(self.transcripts)

        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?
        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS

        transcripts = []
        for transcript in self.transcripts:
            transcripts.append(np.array([self.phonemes.index(i) for i in transcript]))
        # end for
        self.transcripts = transcripts

        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS
        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?
        '''
        You may decide to do this in __getitem__ if you wish.
        However, doing this here will make the __init__ function take the load of
        loading the data, and shift it away from training.
        '''
    # end def


    def __len__(self):
        '''
        What do we return here?
        '''
        return self.length
    # end def

    def __getitem__(self, ind):
        '''
        RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS

        If you didn't do the loading and processing of the data in __init__,
        do that here.

        Once done, return a tuple of features and labels.
        '''

        # torch.FloatTensor is used to conver the mfcc feature vectors into torch
        # tensors
        mfcc = torch.FloatTensor(self.mfccs[ind])
        # torch.LongTensor is a data type that represents tensors containing integer values
        # commonly used for indexing and representing discrete or categorical data
        # used below to convert the phoneme transcriptions from a list of phoneme symbols (strings)
        # to a tensor of integer indices since NNs typically work with numerical data, not strings
        transcript = torch.LongTensor(self.transcripts[ind])
        return mfcc, transcript
    # end def

    def collate_fn(self, batch):
        '''
        Used as a collate function when creating batches
            * Splits the mfccs and transcriptions into
              separate lists
            * Pads the sequences in batch_mfcc and batch_ 
              transcript to have the same length (0's)
            * Computes the length of the original sequences
              before bading and stores them in length_mfccs
              and lengths_transcript
            * Returns the padded mfccs, padded transcriptions
              and lengths of transcription sequences.
        Parameters
        ----------
        batch :
            a list of samples, where each sample is a tuple of
            feature vectors and phoneme transcriptions

        1.  Extract the features and labels from 'batch'
        2.  We will additionally need to pad both features and labels,
            look at pytorch's docs for pad_sequence
        3.  This is a good place to perform transforms, if you so wish.
            Performing them on batches will speed the process up a bit.
        4.  Return batch of features, labels, lenghts of features,
            and lengths of labels.
        '''
        # batch of input mfcc coefficients
        # item = features + labels
        batch_mfcc = [item[0] for item in batch]
        # batch of output phonemes
        batch_transcript = [item[1] for item in batch]


        # HINT: CHECK OUT -> pad_sequence (imported above)
        # Also be sure to check the input format (batch_first)
        # torch.LongTensor is used to create tensors that represent the lengths of sequences
        # in the batch. 
        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)
        # lengths_mfcc = torch.LongTensor([len(sequence) for sequence in batch_mfcc])
        lengths_mfcc = [len(sequence) for sequence in batch_mfcc]

        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True)
        # lengths_transcript = torch.LongTensor([len(sequence) for sequence in batch_transcript])
        lengths_transcript = [len(sequence) for sequence in batch_transcript]

        # You may apply some transformation, Time and Frequency masking, here in the collate function;
        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?
        #                  -> Would we apply transformation on the validation set as well?
        #                  -> Is the order of axes / dimensions as expected for the transform functions?

        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels
        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)
    # end def
# end class

"""### Test Data"""

# Test Dataloader
class AudioDatasetTest(torch.utils.data.Dataset):
    def __init__(self, root, phonemes=PHONEMES, partition="test-clean"):
        """
        Initializes the test data set
        This is only composed on speech recordings
        """
        self.mfcc_dir = os.path.join(root,partition,'mfcc')
        self.mfcc_files = sorted(os.listdir(self.mfcc_dir)) # X
        self.phonemes = phonemes

        self.mfccs = []
        # Iterate through mfccs and transcripts
        for i in range(len(self.mfcc_files)):
        #   Load a single mfcc
            mfcc        = np.load(os.path.join(self.mfcc_dir, self.mfcc_files[i]))
        #   Do Cepstral Normalization of mfcc (explained in writeup)
            mfcc -= np.mean(mfcc, axis=0)
            mfcc /= np.std(mfcc, axis=0)
        #   Append each mfcc to self.mfcc, transcript to self.transcript
            self.mfccs.append(mfcc)

        # WHAT SHOULD THE LENGTH OF THE DATASET BE?
        self.length = len(self.mfccs)    
    # end def

    def __len__(self):
        return self.length
    # end def

    def __getitem__(self, ind):
        '''
        RETURN THE MFCC COEFFICIENTS 

        If you didn't do the loading and processing of the data in __init__,
        do that here.

        Once done, return a tuple of features and labels.
        '''

        # torch.FloatTensor is used to conver the mfcc feature vectors into torch
        # tensors
        mfcc = torch.FloatTensor(self.mfccs[ind])
        return mfcc
    # end def  

    def collate_fn(self,batch):
        '''
        Used as a collate function when creating batches
            * Pads the sequences in batch_mfcc
              to have the same length (0's)
            * Computes the length of the original sequences
              before bading and stores them in length_mfccs
            * Returns the padded mfccs, 
              and lengths of mfccs sequences.
        Parameters
        ----------
        batch :
            a list of samples, where each sample is
            feature vectors 
        1.  Extract the features and labels from 'batch'
        2.  We will additionally need to pad both features and labels,
            look at pytorch's docs for pad_sequence
        3.  This is a good place to perform transforms, if you so wish.
            Performing them on batches will speed the process up a bit.
        4.  Return batch of features, labels, lenghts of features,
            and lengths of labels.
        '''
        # batch of input mfcc coefficients
        # item = features 
        batch_mfcc = batch # [item for item in batch]

        # HINT: CHECK OUT -> pad_sequence (imported above)
        # Also be sure to check the input format (batch_first)
        # torch.LongTensor is used to create tensors that represent the lengths of sequences
        # in the batch. 
        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)
        # lengths_mfcc = torch.LongTensor([len(sequence) for sequence in batch_mfcc])
        lengths_mfcc = [len(sequence) for sequence in batch_mfcc]

        # You may apply some transformation, Time and Frequency masking, here in the collate function;
        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?
        #                  -> Would we apply transformation on the validation set as well?
        #                  -> Is the order of axes / dimensions as expected for the transform functions?

        # Return the following values: padded features, actual length of features
        return batch_mfcc_pad, torch.tensor(lengths_mfcc)
    # end def
# end class      


"""### Config - Hyperparameters"""

root = '/mnt/nvme/home/bbabatun/IDL/HW3P2/11-785-f23-hw3p2/'

# Feel free to add more items here
config = {
    "beam_width" : 10,
    "lr"         : 1e-3,
    "epochs"     : 100,
    "batch_size" : 64,  # Increase if your device can handle it
    "factor": 0.5,
    "dropout": 0.2,
    "patience": 2,
    "mode" : 'min',
    "threshold" : 1e-3,
    "threshold_mode" : 'rel'
}

# You may pass this as a parameter to the dataset class above
# This will help modularize your implementation
transforms = [] # set of tranformations

"""### Data loaders"""

# get me RAMMM!!!!
import gc
gc.collect()

# Create objects for the dataset class
# train should only have transformations
train_data = AudioDataset(root=root)
val_data = AudioDataset(root=root, partition="dev-clean")
test_data = AudioDatasetTest(root=root)

# Do NOT forget to pass in the collate function as parameter while creating the dataloader
train_loader = DataLoader(dataset=train_data,
                          num_workers=4,
                          batch_size=config['batch_size'],
                          pin_memory=True,
                          shuffle=True,
                          collate_fn=train_data.collate_fn)
val_loader = DataLoader(dataset=val_data,
                          num_workers=2,
                          batch_size=config['batch_size'],
                          pin_memory=True,
                          shuffle=False,
                          collate_fn=val_data.collate_fn)
test_loader = DataLoader(dataset=test_data,
                          num_workers=2,
                          batch_size=config['batch_size'],
                          pin_memory=True,
                          shuffle=False,
                          collate_fn=test_data.collate_fn)

print("Batch size: ", config['batch_size'])
print("Train dataset samples = {}, batches = {}".format(train_data.__len__(), len(train_loader)))
print("Val dataset samples = {}, batches = {}".format(val_data.__len__(), len(val_loader)))
print("Test dataset samples = {}, batches = {}".format(test_data.__len__(), len(test_loader)))

# sanity check
for data in train_loader:
    x, y, lx, ly = data
    print(x.shape, y.shape, lx.shape, ly.shape)
    break

"""# NETWORK

## Basic

This is a basic block for understanding, you can skip this and move to pBLSTM one
"""

torch.cuda.empty_cache()


"""## Initialize Basic Network
(If trying out the basic Network)
"""

# torch.cuda.empty_cache()

# model = Network().to(device)
# summary(model, x.to(device), lx) # x and lx come from the sanity check above :)

"""

Without using Batch_First yielded me a good result.


A Bidirectional Long Short-Term Memory (BiLSTM) is a type of RNN
architecture that consists of two separate LSTM layers:
    one processes it from left to right (forward direction)
    other processes it from right to left (backward direction)

The outputs of the two separate LSTM layers are then combined
to make predictions or extract features from the input sequence.
(1) Input sequence is passed to the bLSTM (here its a sequence of
    utterances)
(2) The forward LSTM processes the input sequence from LtoR
    At each time step, it takes the current input vector and
    the previous hidden states as inputs and computes a new
    hidden state and output. (past to present)
(3) The backward LSTM process the input seqeunce from RtoL. 
    At each time step, it takes the current input vector and
    the previous hidden state as inputs and computes a new
    hidden state and output. (future to present)
(4) After outputs from both direction has been calculated, 
    their outputs are concatenated. The result is then fed into
    another RNN layer or a NN for further processing

"""
class FBLSTM(torch.nn.Module):
    def __init__(self, input_size, output_size, hidden_size, num_layers, bidirectional=True, dropout=0.2):
        """
        input_size : size of input vectors (dimension of word embeddings)
        hidden_size : number of LSTM units in each direction
            number of hidden units or neurons in each LSTM layer (64 to 1024)
            increasing allows LSTM to capture more complex patterns
        num_layers : number of LSTM layers
            specifies the number of LSTM layers stacked on top of each other
            increasing num_layers makes the network deeper for modelling more
            intricate relationships (1 to 3)
        num_directions : set of 2 for bidirection
        """
        super(FBLSTM, self).__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_direction = 2 if bidirectional else 1

        # define the bidirectional layer
        self.bilstm = torch.nn.LSTM(input_size=self.input_size, 
                                    hidden_size=self.hidden_size,
                                    num_layers=self.num_layers,
                                    bidirectional=bidirectional,
                                    batch_first=True,
                                    dropout=dropout)
        self.linearclass = torch.nn.Linear(self.hidden_size*self.num_direction,
                                           self.output_size)
        self.logsoftmax = torch.nn.LogSoftmax(dim=2)
    # end def

    def forward(self, x, xlength):
        # batch, T_in, Cin
        # pack sequence after cnn layers
        xpacked = pack_padded_sequence(x, xlength.cpu(), batch_first=True, enforce_sorted=True)
        
        out = self.bilstm(xpacked)[0]
        # It is an inverse operation to pack_padded_sequence.
        out_unpacked, out_length = pad_packed_sequence(out, batch_first=False)
        out_linear = self.linear(out_unpacked) # linear
        out = F.log_softmax(out_linear, dim=2)

        return out, out_length
    # end def
# end class 
         

"""## ASR Network

### Pyramid Bi-LSTM (pBLSTM)
"""

# Utils for network
torch.cuda.empty_cache()

class PermuteBlock(torch.nn.Module):
    def forward(self, x):
        return x.transpose(1, 2)
    # end def
# end class

# https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5
class LockedDropout(torch.nn.Module):
    def __init__(self, dropout=0.5, batch_first=True):
        super().__init__()
        self.dropout = dropout
        self.batch_first=batch_first
    # end def

    def forward(self, x):
        if not self.training or not self.dropout:
            return x
        # end if
        # T, B, C to B,T,C
        x, x_lens = pad_packed_sequence(x, batch_first=self.batch_first)
        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1-self.dropout)
        mask = Variable(m, requires_grad=False)/(1-self.dropout)
        mask = mask.expand_as(x)
        x = mask*x
        x = pack_padded_sequence(x, x_lens, batch_first=self.batch_first, enforce_sorted=False)
        return x
    # end def
# end class

class pBLSTM(torch.nn.Module):
    '''
    Pyramidal BiLSTM
    Read the write up/paper and understand the concepts and then write your implementation here.

    At each step,
    1. Pad your input if it is packed (Unpack it)
    2. Reduce the input length dimension by concatenating feature dimension
        (Tip: Write down the shapes and understand)
        (i) How should  you deal with odd/even length input?
        (ii) How should you deal with input length array (x_lens) after truncating the input?
    3. Pack your input
    4. Pass it into LSTM layer

    To make our implementation modular, we pass 1 layer at a time.
    '''
    def __init__(self, input_size, hidden_size, dropout=0.0, num_layers=3, bidirectional=True, batch_first=True, lockdropout=0.2):
        super(pBLSTM, self).__init__()
        # Initialize a single layer bidirectional LSTM with the given input_size and hidden_size
        self.batch_first=batch_first
        self.blstm = torch.nn.LSTM(input_size=input_size,
                                   hidden_size=hidden_size,
                                   num_layers=num_layers,
                                   bidirectional=bidirectional,
                                   dropout=dropout,
                                   batch_first=batch_first)
        self.dropout = LockedDropout(lockdropout)

    def forward(self, x_packed): # x_packed is a PackedSequence

        # Pad Packed Sequence
        if isinstance(x_packed, PackedSequence):
            x, x_lens = pad_packed_sequence(x_packed, batch_first=self.batch_first)
        else:
            x = x_packed
            x_lens = None
        # end if

        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature 
        # dimensions as mentioned above
        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.
        x, x_lens = self.trunc_reshape(x, x_lens)
        # Pack Padded Sequence. What output(s) would you get?
        if x_lens is not None:
            x = pack_padded_sequence(x, x_lens.cpu(), batch_first=self.batch_first, enforce_sorted=False)
        # end if

        # Pass the sequence through bLSTM
        output, hidden = self.blstm(x)
        output = self.dropout(output)

        # What do you return?

        return output, hidden

    def trunc_reshape(self, x, x_lens):
        # If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)
        # import pdb 
        # pdb.set_trace()
        batch_size, length, dim = x.shape # dim is features
        if length%2 != 0:
            # if odd, trim to even length so we can downsample
            x = x[:,:-1,:]
            x_lens -= 1
            length -= 1
        # end if

        # Reshape x. When reshaping x, you have to reduce number of timesteps by 
        # a downsampling factor while increasing number of features by the same factor
        # x = x.contiguous().view(batch_size, length//2, dim*2)
        x = torch.reshape(x, shape=(batch_size, length//2, 2*dim))

        # Reduce lengths by the same downsampling factor
        x_lens = torch.clamp(x_lens, max=length//2, out=None)
        return x, x_lens
    # # end def
# end pLSTM class


"""### Encoder"""

class Encoder(torch.nn.Module):
    '''
    The Encoder takes utterances as inputs and returns latent feature representations
    '''
    def __init__(self, input_size, encoder_hidden_size, batch_first=True, bidirectional=True, dropout=0.20):
        super(Encoder, self).__init__()
        self.batch_first = batch_first
        #You can use CNNs as Embedding layer to extract features. 
        # Keep in mind the Input dimensions and expected dimension of Pytorch CNN.
        self.embedding = torch.nn.Sequential(
            torch.nn.Conv1d(in_channels=input_size, out_channels=encoder_hidden_size, kernel_size=3, stride=1, padding=1, bias=False),
            torch.nn.BatchNorm1d(encoder_hidden_size),
            torch.nn.GELU(),
            torch.nn.Conv1d(in_channels=encoder_hidden_size, out_channels=encoder_hidden_size*2, kernel_size=3, stride=1, padding=1, bias=False),
            torch.nn.BatchNorm1d(encoder_hidden_size*2),
            torch.nn.GELU(),    
            torch.nn.Conv1d(in_channels=encoder_hidden_size*2, out_channels=encoder_hidden_size*2, kernel_size=3, stride=1, padding=1, bias=False),
            torch.nn.BatchNorm1d(encoder_hidden_size*2),
            torch.nn.GELU(),    
        )

        self.fBLSTM = torch.nn.LSTM(encoder_hidden_size*2,
                                    encoder_hidden_size*2,
                                    num_layers=3,
                                    bidirectional=bidirectional,
                                    batch_first=batch_first)

        # self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?
        #     # Fill this up with pBLSTMs - What should the input_size be?
        #     # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)
        #     # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)
        #     # https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5
        #     # ...
        #     # ... 
        # ) input_size = encoder_hidden_size * 2, hidden_size = encoder_hidden_size * 2)
        # self.pBLSTM = pBLSTM(input_size=encoder_hidden_size*4, hidden_size=encoder_hidden_size*2, dropout=dropout, batch_first=self.batch_first, lockdropout=lockdroput)
        # bi-directional LSTM: forward + backward (X2)
        # p-BLSTM: cuts and shifts down (time window shorter but doubles number of feature)
        self.pBLSTM = torch.nn.ModuleList([pBLSTM(input_size=encoder_hidden_size*2*2*2, hidden_size=encoder_hidden_size*2, dropout=dropout, batch_first=self.batch_first, lockdropout=dropout) for _ in range(1)])


    def forward(self, x, x_lens):
        # Where are x and x_lens coming from? The dataloader
        
        x = torch.transpose(x,1,2)
        x = self.embedding(x)
        x = torch.transpose(x,1,2)
        # Call the embedding layer
        # Pack Padded Sequence
        x = pack_padded_sequence(x,x_lens.cpu(), batch_first=self.batch_first, enforce_sorted=False)
        # Pass Sequence through the pyramidal Bi-LSTM layer
        x, _  = self.fBLSTM(x)
        for pyramidLSTM in self.pBLSTM:
            x, _ = pyramidLSTM.forward(x)
        
        encoder_outputs, encoder_lens = pad_packed_sequence(x, batch_first=self.batch_first)
        # encoder_outputs=x
        # encoder_lens = x_lens
        # import pdb 
        # pdb.set_trace()

        # Remember the number of output(s) each function returns
        # print(encoder_outputs.size)

        return encoder_outputs, encoder_lens
    # end def

"""### Decoder"""

class Decoder(torch.nn.Module):

    def __init__(self, embed_size, hidden_layers, dropout=0.2, output_size= 41):
        super().__init__()
        # [512,1024,1024,1280,1280,1280,1280,1280,1024,512]
        # dropout=0.20
        layers = []
        layers.append(PermuteBlock())
        layers.append(torch.nn.BatchNorm1d(embed_size))
        layers.append(PermuteBlock())
        layers.append(torch.nn.Linear(embed_size,hidden_layers[0]))
        layers.append(torch.nn.ReLU())
        if len(hidden_layers)==1:
            layers.append(torch.nn.Dropout(dropout))
        layers.append(PermuteBlock())
        layers.append(torch.nn.BatchNorm1d(hidden_layers[0]))
        layers.append(PermuteBlock())

        for i in range(len(hidden_layers)-1):
            layers.append(torch.nn.Linear(hidden_layers[i],hidden_layers[i+1]))
            layers.append(torch.nn.ReLU())
            if i%2==0:
                layers.append(torch.nn.Dropout(dropout))
            layers.append(PermuteBlock())
            layers.append(torch.nn.BatchNorm1d(hidden_layers[i+1]))
            layers.append(PermuteBlock())
        # end for
        
        layers.append(torch.nn.Linear(hidden_layers[-1],output_size))
        
        self.mlp = torch.nn.Sequential(*layers)

        self.softmax = torch.nn.LogSoftmax(dim=2)
    # end def

    def forward(self, encoder_out):
        #call your MLP
        #Think what should be the final output of the decoder for the classification
        out = self.mlp(encoder_out)
        out = self.softmax(out)
        return out
    # end def

class ASRModel(torch.nn.Module):

    def __init__(self, input_size, hidden_layers, encoder_hidden_size=64, dropout=0.2, output_size= len(PHONEMES)):
        super().__init__()

        self.augmentations  = torch.nn.Sequential(
            # Add Time Masking/ Frequency Masking
            #Hint: See how to use PermuteBlock() function defined above
            tat.TimeMasking(dropout*100),
            tat.FrequencyMasking(dropout*100)
        )
        # Initialize Encoder
        self.encoder        = Encoder(input_size=input_size, # 28
                                      encoder_hidden_size=encoder_hidden_size,
                                      dropout=dropout) #64
        # Initialize Decoder
        self.decoder        = Decoder(embed_size=encoder_hidden_size*4, 
                                      hidden_layers=hidden_layers, 
                                      dropout=dropout,
                                      output_size=output_size)



    def forward(self, x, lengths_x):

        if self.training:
            x = self.augmentations(x)

        encoder_out, encoder_lens   = self.encoder(x, lengths_x)
        # print('encoder_out shape: ',encoder_out)
        decoder_out                 = self.decoder(encoder_out)
        # print('decoder_out shape: ',decoder_out)

        return decoder_out, encoder_lens

"""## Initialize ASR Network"""
# [512,1024,1024,1280,1280,1280,1280,1280,1024,512]
model = ASRModel(
    input_size  = 28,
    encoder_hidden_size=64,
    hidden_layers=[512,128],
    output_size = len(PHONEMES),
    dropout=config['dropout']
).to(device)
print(model)
# summary(model, x.to(device), lx)

"""# Training Config
Initialize Loss Criterion, Optimizer, CTC Beam Decoder, Scheduler, Scaler (Mixed-Precision), etc.
"""

# Define CTC loss as the criterion. How would the losses be reduced?
criterion = torch.nn.CTCLoss(zero_infinity=True)
# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html
# Refer to the handout for hints

optimizer =  torch.optim.AdamW(model.parameters(), config['lr']) # What goes in here?

# Declare the decoder. Use the CTC Beam Decoder to decode phonemes
# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode
decoder = CTCBeamDecoder(LABELS, beam_width=config['beam_width'], log_probs_input=True)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 
                                                       factor=config['factor'], 
                                                       patience=config['patience'],
                                                       mode=config['mode'],
                                                       threshold=config['threshold'],
                                                       threshold_mode=config["threshold_mode"],
                                                       verbose=True)
# optimizer, mode= config['sc_mode'], factor= config['factor'], 
# patience= config['patience'], threshold= config['sc_threshold'], 
# threshold_mode= config['threshold mode'],  verbose=True)#TODO
# Mixed Precision, if you need it
scaler = torch.cuda.amp.GradScaler()

"""# Decode Prediction"""

def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):

    # look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.
    # print('before change: ')
    # print(output.shape, config['batch_size'])
    if output.shape[0] != config['batch_size']:
        output = torch.transpose(output, 0, 1)
    # print('output from decode_prediction')
    # print(output.shape, output_lens)
    beam_results, _, _, out_seq_len = decoder.decode(output, seq_lens= output_lens) #lengths - list of lengths

    pred_strings                    = []

    for i in range(output_lens.shape[0]):
        # Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.
        beam_slice = beam_results[i,0,:out_seq_len[i,0]]
        pred_str = []
        pred_str = [PHONEME_MAP[i] for i in beam_slice]
        pred_strings.append(pred_str)

    return pred_strings
# end def

def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers

    dist            = 0
    batch_size      = label.shape[0]

    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)

    for i in range(batch_size):
        # Get predicted string and label string for each element in the batch
        pred_string = pred_strings[i]
        label_slice = label[i,0:label_lens[i]]
        label_string = [PHONEME_MAP[k] for k in label_slice]
        dist += Levenshtein.distance(pred_string, label_string)

    dist /= batch_size # Uncomment this, but think about why we are doing this
    return dist
# end def

"""# Test Implementation"""

# test code to check shapes

model.eval()
for i, data in enumerate(val_loader, 0):
    x, y, lx, ly = data
    x, y = x.to(device), y.to(device)
    h, lh = model(x, lx)
    print('hshape:', h.shape)
    h = torch.permute(h, (1, 0, 2))
    print('after permute')
    print(h.shape, y.shape)
    # loss = criterion(torch.permute(h, (1, 0, 2)), y, lh, ly)
    loss = criterion(h, y, lh, ly)
    print('loss')
    print(loss)
    h = torch.permute(h, (1, 0, 2))

    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))

    break

"""# WandB

You will need to fetch your api key from wandb.ai
"""

import wandb
wandb.login()

run = wandb.init(
    name = "early-submission", ## Wandb creates random run names if you skip this field
    reinit = True, ### Allows reinitalizing runs when you re-run this cell
    # run_id = ### Insert specific run id here if you want to resume a previous run
    # resume = "must" ### You need this to resume previous runs, but comment out reinit = True when using this
    project = "hw3p2-ablations", ### Project should be created in your wandb account
    config = config ### Wandb Config for your run
)

"""# Train Functions"""

from tqdm import tqdm

def train_model(model, train_loader, criterion, optimizer):

    model.train()
    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')

    total_loss = 0

    for i, data in enumerate(train_loader):
        optimizer.zero_grad()

        x, y, lx, ly = data
        x, y = x.to(device), y.to(device)

        with torch.cuda.amp.autocast():
            h, lh = model(x, lx)
            h = torch.permute(h, (1, 0, 2))
            loss = criterion(h, y, lh, ly)

        if not torch.isfinite(loss).all():
            print(f'WARNING: non-finite loss, skipping update for batch {i}')
            print(loss)
            print(lh >= ly)
            continue

        total_loss += loss.item()

        batch_bar.set_postfix(
            loss="{:.04f}".format(float(total_loss / (i + 1))),
            lr="{:.06f}".format(float(optimizer.param_groups[0]['lr'])))

        batch_bar.update() # Update tqdm bar

        # Another couple things you need for FP16.
        scaler.scale(loss).backward() # This is a replacement for loss.backward()
        scaler.step(optimizer) # This is a replacement for optimizer.step()
        scaler.update() # This is something added just for FP16

        del x, y, lx, ly, h, lh, loss
        torch.cuda.empty_cache()

    batch_bar.close() # You need this to close the tqdm bar

    return total_loss / len(train_loader)


def validate_model(model, val_loader, decoder, phoneme_map= LABELS):

    model.eval()
    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')

    total_loss = 0
    vdist = 0

    for i, data in enumerate(val_loader):

        x, y, lx, ly = data
        x, y = x.to(device), y.to(device)

        with torch.inference_mode():
            h, lh = model(x, lx)
            h = torch.permute(h, (1, 0, 2))
            loss = criterion(h, y, lh, ly)

        total_loss += float(loss)
        # vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)
        vdist += calculate_levenshtein(h, y, lh, ly, decoder, phoneme_map)

        batch_bar.set_postfix(loss="{:.04f}".format(float(total_loss / (i + 1))), dist="{:.04f}".format(float(vdist / (i + 1))))

        batch_bar.update()

        del x, y, lx, ly, h, lh, loss
        torch.cuda.empty_cache()

    batch_bar.close()
    total_loss = total_loss/len(val_loader)
    val_dist = vdist/len(val_loader)
    return total_loss, val_dist

"""## Training Setup"""

def save_model(model, optimizer, scheduler, metric, epoch, path):
    torch.save(
        {'model_state_dict'         : model.state_dict(),
         'optimizer_state_dict'     : optimizer.state_dict(),
         'scheduler_state_dict'     : scheduler.state_dict(),
         metric[0]                  : metric[1],
         'epoch'                    : epoch},
         path
    )

def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):

    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])

    if optimizer != None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    if scheduler != None:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    epoch   = checkpoint['epoch']
    metric  = checkpoint[metric]

    return [model, optimizer, scheduler, epoch, metric]

# This is for checkpointing, if you're doing it over multiple sessions

last_epoch_completed = 0
start = last_epoch_completed
end = config["epochs"]
best_lev_dist = float("inf") # if you're restarting from some checkpoint, use what you saw there.
epoch_model_path = 'checkpoint.pth' # set the model path( Optional, you can just store best one. Make sure to make the changes below )
best_model_path = 'best_model.pth'# set best model path

torch.cuda.empty_cache()
gc.collect()

#TODO: Please complete the training loop

for epoch in range(0, config['epochs']):

    print("\nEpoch: {}/{}".format(epoch+1, config['epochs']))

    curr_lr = float(optimizer.param_groups[0]['lr'])

    train_loss              = train_model(model, train_loader, criterion, optimizer)
    valid_loss, valid_dist  = validate_model(model, val_loader, decoder)
    scheduler.step(valid_dist)

    print("\tTrain Loss {:.04f}\t Learning Rate {:.07f}".format(train_loss, curr_lr))
    print("\tVal Dist {:.04f}%\t Val Loss {:.04f}".format(valid_dist, valid_loss))


    wandb.log({
        'train_loss': train_loss,
        'valid_dist': valid_dist,
        'valid_loss': valid_loss,
        'lr'        : curr_lr
    })

    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)
    wandb.save(epoch_model_path)
    print("Saved epoch model")

    if valid_dist <= best_lev_dist:
        best_lev_dist = valid_dist
        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)
        wandb.save(best_model_path)
        print("Saved best model")
      # You may find it interesting to exlplore Wandb Artifcats to version your models
run.finish()

"""# Generate Predictions and Submit to Kaggle"""

# Make predictions

# Follow the steps below:
# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams
# 2. Get prediction string by decoding the results of the beam decoder

TEST_BEAM_WIDTH = 20

test_decoder    = CTCBeamDecoder(LABELS, beam_width=TEST_BEAM_WIDTH, log_probs_input=True)
results = []

model.eval()
print("Testing")
for data in tqdm(test_loader):

    x, lx   = data
    x       = x.to(device)

    with torch.no_grad():
        h, lh = model(x, lx)
        h = torch.permute(h, (1, 0, 2))
        

    # call decode_prediction
    prediction_string= decode_prediction(h, lh, test_decoder)
    results.extend(prediction_string)
    # save the output in results array.

    del x, lx, h, lh
    torch.cuda.empty_cache()

results = ["".join(arr) for arr in results]
data_dir = f"{root}/test-clean/random_submission.csv"
df = pd.read_csv(data_dir)
df.label = results
df.to_csv('submission.csv', index = False)

command = "kaggle competitions submit -c automatic-speech-recognition-asr -f submission.csv -m 'I made it!'"
subprocess.run(command, shell=True)


# def init_weights_BLSTM(bidirectional, num_layers, hidden_size,batch_size, device='cuda'):
#     """
#     Initialize the hidden and cell state layers of the bidirectional LSTM

#     Parameters
#     ----------
#     bidirectional : bool
#         True for BLSTM, False for regular LSTM
#     num_layers : int
#         Number of LSTM layers in LSTM part of model
#     hidden_size : int
#         Dimension of each hidden layer in LSTM model
#     batch_size : int
#         Batch size
    
#     Returns
#     -------
#     hidden_state, cell_state
#     """
#     num_direction = 2 if bidirectional else 1

#     # inialize hidde and cell state layers
#     gain = np.sqrt(2)

#     hidden_state = init_weights_BLSTM((num_direction * num_layers, batch_size, hidden_size))
#     cell_state = init_weights_BLSTM((num_direction * num_layers, batch_size, hidden_size))

#     hidden_state = hidden_state.to(device)
#     cell_state = cell_state.to(device)

#     return hidden_state, cell_state
# # end def


# def initialize_lstm_weight(shape):
#     """
#     Initialize an orthogonal random array with given shape.
#     :param shape (Tuple): defines the shape of the array
#     :return: np.array
#     """
#     gain = np.sqrt(2)  # relu (default gain = 1.0)
#     w = sample(shape, gain)
#     return torch.FloatTensor(w)

# # def get_output_size(input_size, padding, dilation, kernel_size, stride):
# #     """
# #     Calculates output size
# #     """

# #     input_padded = input_size + 2 * padding
# #     dilated_kernel = (kernel_size - 1) * (dilation - 1) + kernel_size
# #     output_size = (input_padded - dilated_kernel)//stride + 1

# #     return output_size
# # # end def